{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKaD29tpdNjj"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7JNPLUadgNH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "import sys\n",
    "sys.path.append('drive/gdrive/MyDrive/training_set.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ttX_4IzdiEY"
   },
   "outputs": [],
   "source": [
    "with open('training_set.json','rb') as f:\n",
    "     squad = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90i5YFHFdmxL"
   },
   "source": [
    "UNDERSTADING THE  DATA STRUCTURE AND TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouHwubmrdjp5"
   },
   "outputs": [],
   "source": [
    "type(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNKnJkGldsWV"
   },
   "outputs": [],
   "source": [
    "squad.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5k_wqmoduH0"
   },
   "outputs": [],
   "source": [
    "squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuXVWgFNdvsp"
   },
   "outputs": [],
   "source": [
    "type(squad['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta98XgGVdyYi"
   },
   "outputs": [],
   "source": [
    "print(len(squad['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ae68biSVd0Ul"
   },
   "outputs": [],
   "source": [
    "squad['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQiEEY-ed2aA"
   },
   "outputs": [],
   "source": [
    "type(squad['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULGulMcBd5Ji"
   },
   "outputs": [],
   "source": [
    "squad['data'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fs_w1wud6ng"
   },
   "outputs": [],
   "source": [
    "type(squad['data'][1]['title'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIevRYrBd9i2"
   },
   "outputs": [],
   "source": [
    "type(squad['data'][0]['paragraphs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiN4yuQLd-TB"
   },
   "outputs": [],
   "source": [
    "squad['data'][0]['paragraphs'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQaBR-ymeCXQ"
   },
   "outputs": [],
   "source": [
    "type(squad['data'][0]['paragraphs'][0]['context'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRzZ9mCaeC9y"
   },
   "outputs": [],
   "source": [
    "type(squad['data'][0]['paragraphs'][0]['qas'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WsqJskReE_O"
   },
   "outputs": [],
   "source": [
    "squad['data'][0]['paragraphs'][0]['qas'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7psst71deHhc"
   },
   "outputs": [],
   "source": [
    "type(squad['data'][0]['paragraphs'][0]['qas'][0] ['answers'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbTTAgwqeJ9Z"
   },
   "outputs": [],
   "source": [
    "squad['data'][0]['paragraphs'][0]['qas'][0] ['answers'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fH1CU0p5eL0_"
   },
   "outputs": [],
   "source": [
    "type(squad['data'][0]['paragraphs'][0]['qas'][0] ['id'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlqnIEqYeNez"
   },
   "outputs": [],
   "source": [
    "type(squad['data'][0]['paragraphs'][0]['qas'][0] ['question'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYEUcJRWeSnx"
   },
   "source": [
    "CONSOLIDATION OF DATA TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42o3-INFeVTk"
   },
   "source": [
    "# POINT 1 : 'squad' is a complicated dictionary with two keys 'data' and 'version'. \n",
    "\n",
    "# POINT 2 : With respect to our work at hand, the value of the key'version' need not play any role at all. \n",
    "\n",
    "# POINT 3:  All expected analysis, will spin around the value corresponding to the 'data' key.\n",
    "\n",
    "# POINT 4: The value corresponding to 'data' key is itself a list comprising 442 elements. \n",
    "#          Since we need to split the given data set into training and validation, it is at the 'data' key level we possibly\n",
    "#          have to do the splitting.\n",
    "\n",
    "# POINT 5: The 'data', which is of type list, the elements of that list is again a dictionary having two keys \n",
    "#          'title' and 'paragraphs'.\n",
    "\n",
    "# POINT 6: 'title' is of type string whereas'paragraphs' is again of type dictionary with two keys 'context' and 'qas'.\n",
    "\n",
    "# POINT 7:  'context' is of type strings and 'qas' is again a dictionary.\n",
    "\n",
    "# POINT 8: The dictionary 'qas' contains three keys namely, 'answers','question' and 'id'.'id' and 'questions' are strings \n",
    "#          whereas 'answers' is again another dictionary with keys 'answer_start' and 'text'.\n",
    "\n",
    "# POINT 9 : Also note, in the original data set we do not have any answer_end index as would be required as we use BERT or variants for building the Q/A application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1VwinEcejEE"
   },
   "source": [
    "Extracting the Contexts, Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS6Jbvr2ePM4"
   },
   "outputs": [],
   "source": [
    "# Refer : https://huggingface.co/transformers/v4.0.1/custom_datasets.html\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('training_set.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPa5UpgKen7U"
   },
   "outputs": [],
   "source": [
    "for group in squad['data']:\n",
    "    print(group['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMWJWs8Uev0k"
   },
   "source": [
    "Printing the Contexts, Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgGmjZYxeqJe"
   },
   "outputs": [],
   "source": [
    "# Note the repeatation of Context in the contexts list\n",
    "\n",
    "train_contexts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2wRqcXKe0oT"
   },
   "outputs": [],
   "source": [
    "train_answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khElPeGte20C"
   },
   "outputs": [],
   "source": [
    "train_questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFrVYeE6e4mW"
   },
   "outputs": [],
   "source": [
    "len(train_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJuYg6OXe7S8"
   },
   "outputs": [],
   "source": [
    "len(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euyytjbse7bI"
   },
   "outputs": [],
   "source": [
    "len(train_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the length of all the 3 lists is same, this means for each question associated with one context, there is ONLY one answer but the output of the code train_context[:25] reveals that for each context there can be several associated questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGThmFENfC1k"
   },
   "source": [
    "Computing the end index (To understand end of answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXRXTCqae_Ko"
   },
   "outputs": [],
   "source": [
    "# Refer : https://huggingface.co/transformers/v4.0.1/custom_datasets.html\n",
    " \n",
    "def add_end_index(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_index = start_idx + len(gold_text)\n",
    "        answer['answer_end'] = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lARgX_a-fGWC"
   },
   "outputs": [],
   "source": [
    "add_end_index(train_answers,train_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mekvWuctfIeM"
   },
   "outputs": [],
   "source": [
    "print(train_answers[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCuGXMGpfNLN"
   },
   "source": [
    "Installing Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMMwHUI5fKNf"
   },
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRDfpbmbfTsj"
   },
   "source": [
    "Performing Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eU9mQ-axfQJY"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_contexts,train_questions,padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context-question pairs are now being represented as Encoding objects. These objects concatenate each corresponding context and question strings. Each context and question pair is separated with a [SEP] token.\n",
    "This concatenated version is stored within the 'input_ids' attribute of the Encoding object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the encoded output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the decoded output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KepmeUgKfW6e"
   },
   "outputs": [],
   "source": [
    "#Refer : https://huggingface.co/transformers/v4.0.1/custom_datasets.html\n",
    "\n",
    "tokenizer.decode(train_encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWlXlTDgfZKY"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(train_encodings['input_ids'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiDhmiusfeEh"
   },
   "source": [
    "ADD TOKEN POSITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_encodings['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization done prior, takes care of the Context and Question pair but nothing has been done as yet for the start and end positions of the answers for each question associated with a certain context. To achieve this, the below function has been designed.\n",
    "\n",
    "To achieve this, the in-built char_to_token method has been utilized.\n",
    "\n",
    "train_answers is a list, each element of that list being a dictionary with 3 keys namely, answer_start, text and answer_end. We iterate over this list and access the start and end positions of each answer using the necessary keys and append the same to two different lists namely start_positions and end_positions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answers[0]['answer_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answers[0]['answer_end']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since blank space is returned, so we substract 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.char_to_token(0,train_answers[0]['answer_end']-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.char_to_token(0,train_answers[0]['answer_start'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 541-515 is dependent on the length of the answer and we can have an idea from this difference as how long was the answer itself.However, the numbers produced via tokenization is just some kind of bijection and apparently nothing to do with the answer length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGTdCbnefarp"
   },
   "outputs": [],
   "source": [
    "#Refer : https://huggingface.co/transformers/v4.0.1/custom_datasets.html\n",
    "#Refer: https://huggingface.co/transformers/v4.0.1/main_classes/tokenizer.html#transformers.BatchEncoding.char_to_token\n",
    "\n",
    "\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        # if None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that two more attributes namely 'start_positions' and 'end_positions' got added to the encoding objects.\n",
    "\n",
    "Each of these is simply a list containing the start and end token positions of the answer that corresponds to the respective question-context pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1DB2LuIfh9h"
   },
   "outputs": [],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer : https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "\n",
    "Refer : https://www.tensorflow.org/guide/data\n",
    "\n",
    "The tf.data.Dataset API supports writing descriptive and efficient input pipelines. Dataset usage follows a common pattern:\n",
    "\n",
    "Create a source dataset from your input data.\n",
    "Apply dataset transformations to preprocess the data.\n",
    "Iterate over the dataset and process the elements.\n",
    "Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QaKNZOGfpTR"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
