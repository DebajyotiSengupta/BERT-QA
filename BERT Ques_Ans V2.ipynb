{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKaD29tpdNjj"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "sys.path.append('drive/gdrive/MyDrive/training_set.json')"
      ],
      "metadata": {
        "id": "a7JNPLUadgNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_set.json','rb') as f:\n",
        "     squad = json.load(f)"
      ],
      "metadata": {
        "id": "6ttX_4IzdiEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNDERSTADING THE  DATA STRUCTURE AND TYPE"
      ],
      "metadata": {
        "id": "90i5YFHFdmxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad)"
      ],
      "metadata": {
        "id": "ouHwubmrdjp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad.keys()"
      ],
      "metadata": {
        "id": "WNKnJkGldsWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad"
      ],
      "metadata": {
        "id": "L5k_wqmoduH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad['data'])"
      ],
      "metadata": {
        "id": "AuXVWgFNdvsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(squad['data']))"
      ],
      "metadata": {
        "id": "ta98XgGVdyYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad['data']"
      ],
      "metadata": {
        "id": "Ae68biSVd0Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad['data'][0])"
      ],
      "metadata": {
        "id": "uQiEEY-ed2aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad['data'][0].keys()"
      ],
      "metadata": {
        "id": "ULGulMcBd5Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad['data'][1]['title'][0])"
      ],
      "metadata": {
        "id": "1fs_w1wud6ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad['data'][0]['paragraphs'][0])"
      ],
      "metadata": {
        "id": "bIevRYrBd9i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad['data'][0]['paragraphs'][0].keys()"
      ],
      "metadata": {
        "id": "eiN4yuQLd-TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad['data'][0]['paragraphs'][0]['context'][0])"
      ],
      "metadata": {
        "id": "PQaBR-ymeCXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad['data'][0]['paragraphs'][0]['qas'][0])"
      ],
      "metadata": {
        "id": "oRzZ9mCaeC9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad['data'][0]['paragraphs'][0]['qas'][0].keys()"
      ],
      "metadata": {
        "id": "9WsqJskReE_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad['data'][0]['paragraphs'][0]['qas'][0] ['answers'][0])"
      ],
      "metadata": {
        "id": "7psst71deHhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad['data'][0]['paragraphs'][0]['qas'][0] ['answers'][0].keys()"
      ],
      "metadata": {
        "id": "hbTTAgwqeJ9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad['data'][0]['paragraphs'][0]['qas'][0] ['id'][0])"
      ],
      "metadata": {
        "id": "fH1CU0p5eL0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(squad['data'][0]['paragraphs'][0]['qas'][0] ['question'][0])"
      ],
      "metadata": {
        "id": "OlqnIEqYeNez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONSOLIDATION OF DATA TYPE"
      ],
      "metadata": {
        "id": "ZYEUcJRWeSnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POINT 1 : 'squad' is a complicated dictionary with two keys 'data' and 'version'. \n",
        "\n",
        "# POINT 2 : With respect to our work at hand, the value of the key'version' need not play any role at all. \n",
        "\n",
        "# POINT 3:  All expected analysis, will spin around the value corresponding to the 'data' key.\n",
        "\n",
        "# POINT 4: The value corresponding to 'data' key is itself a list comprising 442 elements. \n",
        "#          Since we need to split the given data set into training and validation, it is at the 'data' key level we possibly\n",
        "#          have to do the splitting.\n",
        "\n",
        "# POINT 5: The 'data', which is of type list, the elements of that list is again a dictionary having two keys \n",
        "#          'title' and 'paragraphs'.\n",
        "\n",
        "# POINT 6: 'title' is of type string whereas'paragraphs' is again of type dictionary with two keys 'context' and 'qas'.\n",
        "\n",
        "# POINT 7:  'context' is of type strings and 'qas' is again a dictionary.\n",
        "\n",
        "# POINT 8: The dictionary 'qas' contains three keys namely, 'answers','question' and 'id'.'id' and 'questions' are strings \n",
        "#          whereas 'answers' is again another dictionary with keys 'answer_start' and 'text'.\n",
        "\n",
        "# POINT 9 : Also note, in the original data set we do not have any answer_end index as would be required as we use BERT or variants for building the Q/A application\n"
      ],
      "metadata": {
        "id": "42o3-INFeVTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the Contexts, Questions and Answers"
      ],
      "metadata": {
        "id": "D1VwinEcejEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Refer : https://huggingface.co/transformers/v4.0.1/custom_datasets.html\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "train_contexts, train_questions, train_answers = read_squad('training_set.json')"
      ],
      "metadata": {
        "id": "iS6Jbvr2ePM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for group in squad['data']:\n",
        "    print(group['title'])"
      ],
      "metadata": {
        "id": "TPa5UpgKen7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the Contexts, Questions and Answers"
      ],
      "metadata": {
        "id": "yMWJWs8Uev0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the repeatation of Context in the contexts list\n",
        "train_contexts[:10]"
      ],
      "metadata": {
        "id": "EgGmjZYxeqJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_answers[:10]"
      ],
      "metadata": {
        "id": "B2wRqcXKe0oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_questions[:10]"
      ],
      "metadata": {
        "id": "khElPeGte20C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_contexts)"
      ],
      "metadata": {
        "id": "gFrVYeE6e4mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_questions)"
      ],
      "metadata": {
        "id": "zJuYg6OXe7S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_answers)"
      ],
      "metadata": {
        "id": "euyytjbse7bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the end index (To understand end of answers)"
      ],
      "metadata": {
        "id": "oGThmFENfC1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Refer : https://huggingface.co/transformers/v4.0.1/custom_datasets.html\n",
        " \n",
        "def add_end_index(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_index = start_idx + len(gold_text)\n",
        "        answer['answer_end'] = end_index"
      ],
      "metadata": {
        "id": "EXRXTCqae_Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_end_index(train_answers,train_contexts)"
      ],
      "metadata": {
        "id": "lARgX_a-fGWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_answers[:10])"
      ],
      "metadata": {
        "id": "mekvWuctfIeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Transformers"
      ],
      "metadata": {
        "id": "ZCuGXMGpfNLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "id": "XMMwHUI5fKNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Encoding"
      ],
      "metadata": {
        "id": "nRDfpbmbfTsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "train_encodings = tokenizer(train_contexts,train_questions,padding=True,truncation=True)"
      ],
      "metadata": {
        "id": "eU9mQ-axfQJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Refer : https://huggingface.co/docs/transformers/preprocessing\n",
        "tokenizer.decode(train_encodings['input_ids'][0])"
      ],
      "metadata": {
        "id": "KepmeUgKfW6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(train_encodings['input_ids'][5])"
      ],
      "metadata": {
        "id": "yWlXlTDgfZKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADD TOKEN POSITIONS"
      ],
      "metadata": {
        "id": "jiDhmiusfeEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Refer : https://huggingface.co/docs/transformers/preprocessing\n",
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "        # if None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)"
      ],
      "metadata": {
        "id": "BGTdCbnefarp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings.keys()"
      ],
      "metadata": {
        "id": "X1DB2LuIfh9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)"
      ],
      "metadata": {
        "id": "_tkQFLUofj5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForQuestionAnswering\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "5QaKNZOGfpTR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}